{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_-Q8naoDk36"
      },
      "source": [
        "# Neural Networks in PyTorch\n",
        "\n",
        "\n",
        "We will build a dense neural network in `pyTorch` to solve a simple regression problem. \n",
        "\n",
        "**Advanced task:** If you are already an expert with neural networks and `pyTorch`, I recommend trying to hand-code a neural netork using `Jax` (if you have not yet tried this). There is *amazing*, cutting edge work being done with deep learning and `Jax` for science applications... ask me about it!  \n",
        "\n",
        "## Learning Task\n",
        "We will construct a dense neural network to predict xyz.\n",
        "\n",
        "*Note that this task does not require (or gain any use from) machine learning. We choose a task with a known mapping to help us create, debug, and tune our first neural network.*\n",
        "\n",
        "## Dataset\n",
        "We will synthesize a simple dataset for our first neural network. The results will be easy to visualize and thus validate that we are building and training our first network correctly.\n",
        "\n",
        "<!--This dataset is a collection of simulated particle events from [Pythia](http://home.thep.lu.se/~torbjorn/Pythia.html). The dataset is a 2D array where each row represents one event from an $e^{-} + p$ collision. This dataset is comprised _only_ of events where exactly 16 particles are produced from an electron-proton collision. Each particle contains $(p_x,p_y,p_z,E,q)$. Each event is therefore represented by 80 numbers. -->\n",
        "\n",
        "\n",
        "<!--**Advanced activity:** There are more interesting event-wise learning tasks using this dataset. Consider crafting your own learning task and target for this data. -->\n",
        "\n",
        "\n",
        "\n",
        "## Computational Notes\n",
        "\n",
        "If this is your first time in a Jupyter-like environment, please read the following carefully:\n",
        "\n",
        " - You are in an active kernel\n",
        " - Run each cell with `Shift + Enter`\n",
        " - You must execute the cells in the order that you want the code to run\n",
        " - `Runtime`$\\rightarrow$`Change runtime type` allows you to utilize GPUs and TPUs. They are unnecessary here, but will become vital in later exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfL6KNLSDk3-",
        "outputId": "1707f676-3e3d-42e7-f6d8-92ca13aca75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import the packages we will be using\n",
        "import numpy as np # vectorized arrays\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "\n",
        "# pytorch packages for neural networks\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "# for future reference, below is how check that you can access the GPU, \n",
        "# but we don't want to use that here, we won't gain any performance.\n",
        "\"\"\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\"\"\"\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTx3KTNYGyQu",
        "outputId": "8842f811-432e-48f5-80d4-fa57a748a062"
      },
      "source": [
        "# Generate sythetic data\n",
        "\n",
        "Generate $10^3$ **noisy** examples from the equation $y = 10(x-0.4)^5$ where $x\\sim U[0,1]$. \n",
        "\n",
        "`pyTorch` requires data to follow the data format I showed in the slides, meaning each *example* is a *row* of data.\n",
        "\n",
        "*Note:* I added Gaussian noise with $\\sigma = 0.01$ to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-GfZs9bI7Km"
      },
      "outputs": [],
      "source": [
        "# generate the data here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O8n_nwTDk3_"
      },
      "source": [
        "Now, we would typically normalize our data. I chose a dataset that is within a sufficient range with a sufficient distribution of values so we can forgoe this step here.\n",
        "\n",
        "We also need to create our train/test/validation datasets. `scikit-learn` (`import sklearn`) comes with nice functions (`sklearn.model_selection.train_test_split()`) for shuffling and splitting data, or you could do it by hand.\n",
        "\n",
        "Additionally, let's turn our `numpy` arrays into `torch` tensors and create a `TensorDataset` and `DataLoader`. Example syntax for (approriately shaped) `numpy` arrays are in a comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "067guDXZDk4A"
      },
      "outputs": [],
      "source": [
        "# create tensor data here\n",
        "\n",
        "\n",
        "#train_x = torch.tensor(t_x)\n",
        "#train_data = TensorDataset(x_data,y_data) # create your datset\n",
        "#train_dataloader = DataLoader(train_data, batch_size=32) # create your dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_YWTBO0Dk4B"
      },
      "source": [
        "# Building our neural network\n",
        "\n",
        "This model has one input and one output. Let's begin by builing a dense, fully connected, regression neural network with **one hidden layer that contains two nodes** (yah, not a very flexible network, but we want to practice tuning). We will begin with a Sigmoid activation function\n",
        "\n",
        "To do this, we will inherit from the `torch.nn.Module` class. Look at the [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) documentation for guidance.\n",
        "\n",
        "Often this is in a python module that may be imported into the notebook for interactive training, as we will do here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap3EizhuDk4B"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    '''\n",
        "    A dense, fully connected neural network.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Constructor. Sets up the architecture.\n",
        "        '''\n",
        "        # complete me\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass through the network.\n",
        "        arguments:\n",
        "            x: TENSOR of inputs to network\n",
        "        returns:\n",
        "            TENSOR of outputs to network\n",
        "        '''\n",
        "        # complete me\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kprt9pbPDk4D"
      },
      "source": [
        "We will create an *instance* of this class and print some information about the model.\n",
        "\n",
        "As we did in the lecture, I like to count the number of trainable parameters for small netwroks to verify that I have built what I *think* I built :) Therefore, please draw out this small network and count your paramters before contiuing onto this next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZkF1erNDk4E",
        "outputId": "c8e8c5a6-c0df-478d-ea4c-847af80ab843"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'NN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m() \u001b[38;5;66;03m# create instance of NN\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print my architecture information\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
          ]
        }
      ],
      "source": [
        "model = NN() # create instance of NN\n",
        "\n",
        "# print my architecture information\n",
        "print(model)\n",
        "print()\n",
        "# counting my trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable parameters:\", total_params)\n",
        "print()\n",
        "\n",
        "# cycle through and print the params \n",
        "# this is more useful when there are more than one inputs or deeper networks\n",
        "for parameter in model.parameters():\n",
        "    print(parameter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VYyii0yDk4F"
      },
      "source": [
        "# Training and Validation\n",
        "\n",
        "We now need to define the training and validation functions. Again, this code could be offloaded into a module, but we will write it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHL4sU3ZDk4G"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Defines a training loop for one epoch (one pass through the training data)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    model.train() # enter train mode\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        \n",
        "        # make a forward pass through the network\n",
        "        pred = model(X) \n",
        "\n",
        "        # Compute prediction loss\n",
        "        lossf = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation:\n",
        "        optimizer.zero_grad() # we don't want to accumulate grads from previous batch\n",
        "        lossf.backward() # back prop through networks\n",
        "        optimizer.step() # update weights\n",
        "\n",
        "        # Sometimes we want some feedback within an epoch\n",
        "        # if batch % 10 == 0: \n",
        "        #     loss, current = lossf.item(), batch * len(X)\n",
        "        #     #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return lossf.detach().numpy()\n",
        "\n",
        "def val(dataloader, model, loss_fn):\n",
        "  \"\"\"\n",
        "  Defines a validation loop for one pass through validation data.\n",
        "  \"\"\"\n",
        "\n",
        "  # complete me. Use the train function for guidance\n",
        "  # Hint: `with torch.no_grad()` disables gradient calculation...\n",
        "  # we def don't want validation gradients leaking into our training!\n",
        "\n",
        "   \n",
        "  return val_loss\n",
        "\n",
        "\n",
        "def test_preds(dataloader, model, loss_fn):\n",
        "    \"\"\"\n",
        "    Code for making predictions from a trained model.\n",
        "\n",
        "    returns: ARRAY of predictions\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return x, yhat\n",
        "\n",
        "def weight_reset(m):\n",
        "  \"\"\"\n",
        "  initialize weights in network\n",
        "  \"\"\"\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "      m.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-MXhgPQDk4H"
      },
      "source": [
        "The last thing we need to enable training is to define our loss function and our optimizer. We went over stochastic gradient descent (SGD) during the lecture, but there are many modifications of SGD. Adam is particularly popular for neural network training. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_MsoqFvDk4H",
        "outputId": "6eb91415-d7b7-43fe-b5e2-9cc231601d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=6, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=6, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "<bound method Module.parameters of MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=6, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=6, out_features=1, bias=True)\n",
            "  )\n",
            ")>\n"
          ]
        }
      ],
      "source": [
        "# define loss function\n",
        "loss_fn = nn.#complete me\n",
        "# define optimizer\n",
        "optimizer = torch.optim.#complete me\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFb0E6GtSK7Z"
      },
      "source": [
        "# Train your network!\n",
        "\n",
        "To start, train your regression network for 10 epochs with a batch size of 128, an Adam optimizer with a learning rate of 0.1, using mean squared error loss. **This is not ideal.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrIo4jsqDk4J",
        "outputId": "fd4b96a9-c89f-4f88-ab83-5d3fa180a859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0212,  0.1496,  0.2247,  0.2913,  0.0744],\n",
            "        [-0.2727,  0.1406,  0.1560,  0.3364,  0.4177],\n",
            "        [-0.3213, -0.2507,  0.1224,  0.3617,  0.0757],\n",
            "        [ 0.3521, -0.3132, -0.0040,  0.2971, -0.3568],\n",
            "        [ 0.1461,  0.1133, -0.2353, -0.0120,  0.0987],\n",
            "        [-0.4193,  0.4288,  0.2096,  0.2203,  0.3387]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1111,  0.3128,  0.4089, -0.2211, -0.2969, -0.1644],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2332,  0.3181, -0.0131,  0.2043, -0.3838, -0.1753]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0927], requires_grad=True)\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: tensor(0.5858, grad_fn=<MseLossBackward0>)\n",
            "val loss: 0.570015705241182\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "train loss: tensor(0.0643, grad_fn=<MseLossBackward0>)\n",
            "val loss: 0.054213462546229746\n",
            "Done!\n",
            "Parameter containing:\n",
            "tensor([[-0.0377,  0.1959,  0.1504,  0.2272, -0.0117],\n",
            "        [-0.3287,  0.2028,  0.0942,  0.2615,  0.3851],\n",
            "        [-0.2707, -0.3206,  0.2160,  0.4385,  0.1751],\n",
            "        [ 0.3084, -0.2473, -0.1109,  0.2131, -0.3827],\n",
            "        [ 0.0887,  0.0644, -0.2166, -0.0036,  0.0878],\n",
            "        [-0.3279,  0.3864,  0.2373,  0.3258,  0.3407]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0815,  0.2518,  0.4514, -0.2826, -0.2633, -0.0543],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.1617,  0.2658, -0.0823,  0.1255, -0.3278, -0.2060]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0607], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "restart = True\n",
        "\n",
        "\n",
        "if restart:\n",
        "  model.apply(weight_reset)\n",
        "  loss = []\n",
        "  val_loss = []\n",
        "        \n",
        "\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for i in range(epochs):\n",
        "   # train your model and keep track of your losses for your learning curve\n",
        "   \n",
        "    if i % 10 == 0:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        print(\"train loss:\",lo)\n",
        "        print(\"val loss:\",vlo)\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcg4Qj1wZDxE"
      },
      "source": [
        "A learning curve will help us determine over and underfitting, which, in turn, helps us tune our model and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "RfhycMMXZahi",
        "outputId": "7fc36189-6056-4dc0-bc13-765e64555a7c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot learning curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to make predictions from our trained model and plot the results.\n"
      ],
      "metadata": {
        "id": "7VDQ4JVYxc0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdrpoGvz-ie0",
        "outputId": "683ad94d-e04e-479a-d920-6562ea706b12"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot test set predictions and test set data on the same plot here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3L0T1uvwxbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rot49hATDk4K"
      },
      "source": [
        "## Tuning\n",
        "The model is learning, but we can do better. \n",
        "\n",
        "**At this point, you should play with the above code to change hyperparameters and note changes to your results.**\n",
        "\n",
        "Alternatively, you can reference the step through tuning of a model below. The step-through tuning is from a more complicated learning problem (that I realized was kinda dumb so I changed it).\n",
        "\n",
        "If you train the model effectively with time to spare, you can generate more challenging synthetic regression data. I multiplied the exisiting function by `np.sin(100*x)` as my next trial. Adding more inputs and or outputs is also a fun modification. \n",
        "\n",
        "\n",
        "## Walk-through tuning:\n",
        "Perhaps we did not have enough model parameters to accurately represent the mapping. Remedy this by increasing the number of hidden nodes to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3SfAb-mDk4L"
      },
      "outputs": [],
      "source": [
        "#Use the same code as in the previous cell\n",
        "#simply change the number of neurons in the hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWoQKonDk4L"
      },
      "source": [
        "We see that we got little improvement here. Another hyperparameter to adjust is *batch size*, which is the number of training examples used to calculate the gradient on each step. While you may initially think that a higher batch size leads to faster or more accurate training, in practice this is not true. The \"noise\" that arises from using less training examples at each iteration can actually help find the global minimum of the loss function.\n",
        "(See here for more info: https://arxiv.org/pdf/1609.04836.pdf)\n",
        "\n",
        "Try decreasing the batch size to 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB9__PwjDk4M"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4QGMh3-Dk4M"
      },
      "source": [
        "This is starting to do better but has significant room for improvement.\n",
        "\n",
        "Another hyperparameter to tune is the *learning rate*. \n",
        "\n",
        " - If the learning rate is too high, we are taking too large of a step in the gradient descent at each iteration and will miss narrow minima in the loss function. \n",
        " - If the learning rate is too small, then we are not traveling far enough in each iteration and we will take far too long to reach a minimum. \n",
        "\n",
        "Perhaps the learning rate is too high and the network can't fine tune. Try decreasing the learning rate to 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s851AM6GDk4N"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh748SgrDk4O"
      },
      "source": [
        "This is not really that much better, but now there is evidence of *overtraining* or *overfitting* -- the training loss is so much lower than the validation loss. \n",
        "\n",
        "A common fix to this is adding *dropout layers*. Try adding a dropout layer with dropout rate of 0.5. https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
        "\n",
        "You can also try batch normalization: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO_Ou80xDk4O"
      },
      "outputs": [],
      "source": [
        "#Dropout layers are located under tf.keras.layers. \n",
        "#They take the dropout rate as their only argument.\n",
        "#BatchNormalization layers are also under tf.keras.layers, and in the simplest use case, take no arguments\n",
        "\n",
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owd0k7kEDk4P"
      },
      "source": [
        "This clearly stopped the overtraining problem, but it still isn't training well. Now, try training on the full dataset with a more reasonable validation split of 0.2. Use a single hidden layer with 20 neurons, a learning rate of 0.001, and a batch size of 256. Just run it for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iStR0aT5Dk4P"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vkQ1kX2Dk4Q"
      },
      "source": [
        "This clearly resulted in a significant improvement and shows how important having a large enough dataset is. Moving on to the choice in activation functions, ReLU is not the only available choice, although it is one of the most popular ones currently. Try training a network using a sigmoid or tanh activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3qtiBmiDk4Q"
      },
      "outputs": [],
      "source": [
        "#Simply change relu to sigmoid or tanh to change the activation function\n",
        "\n",
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRlAj8KFDk4R"
      },
      "source": [
        "Next, try adding 2 new hidden layers to the network. Use the ReLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJlPFnsjDk4R"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3lUeX2dDk4S"
      },
      "source": [
        "Clearly, adding more layers helps improve the quality of the network. There is a limit to how effective this is though. Try having 5 hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kKOzBCrDk4S"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psJeOPOsDk4S"
      },
      "source": [
        "Now, see what happens when you increase the number of neurons per layer from 20 to 50 in the 3 hidden layer model. Consider how they perform compared to ReLU now. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k430N-vIDk4T"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntl38Zo7Dk4U"
      },
      "source": [
        "Try using the sigmoid and the tanh activation functions again and compare them to ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn1MKIiDk4V"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-HNQQhpDk4V"
      },
      "source": [
        "This difference in performance, especially with the sigmoid function, is known as the vanishing gradient problem. If the value for any one the neurons gets too far away from 0, the gradient for sigmoid and tanh gets really close to 0. This means that for deeper networks it is much more difficult to update the weights in the earlier layers as their gradient is so small. Now, remove the fifth column from the input data, the charge, and see what happens when training. Why do you think including charge has such a large impact?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceRTx4CpDk4W"
      },
      "outputs": [],
      "source": [
        "#Complete me:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IscwBDy7Dk4W"
      },
      "source": [
        "Finally, there are other options for the loss function. Try experimenting with alternatives to mean squared error.\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "You can also try some other optimizers -- for example, sgd (with and without momentum), rmsprop, adagrad, adadelta, adamax, and nadam. https://pytorch.org/docs/stable/optim.html#algorithms\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCm5VSXDDk4X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lecture1-NN-torch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}